This README file describes the analog precipitation post-processing methodology 
developed by Tom Hamill at NOAA ERSL/PSD over a number of years.  The application
is set up to use GEFS v2 reforecasts (circa 2012-era NCEP GEFS ensemble system) 
and 1/8-degree Climatology Calibrated Precipitation Analyses of Hou et al. 
(http://journals.ametsoc.org/doi/abs/10.1175/JHM-D-11-0140.1), processed through the
analog methodology described in Hamill, T. M., M. Scheuerer, and G. T. Bates, 2015: 
Analog probabilistic precipitation forecasts using GEFS Reforecasts and Climatology-
Calibrated Precipitation Analyses.  Mon. Wea. Rev., 143, 3300-3309.  

The intent of sharing this code is to set up a structure for the post-processing 
of precipitation (in Fortran) and to make it relatively easy for others with 
alternative post-processing methodologies to adapt the code, swapping out the
analog code for a method of their own choice, but otherwise using the rest of the
infrastructure, for visualization of the forecasts, for generation of summary 
scores (reliability diagrams, Brier Skill Scores), and so forth.

The code herein is a mix of python and fortran, with the fortran doing the heavy
lifting, i.e. the intense computations, and python for visualization and 
packaging up netCDF files and such.

Running the analog code:
=======================

The driving program herein is ppn_analog_ccpa_supplocns.x, which is compiled with
the makefile make_ppn_analog_ccpa_supplocns . It uses various input data, including
reforecast and CCPA data pre-packaged into a netCDF file (see python routine 
precip_forecast_ccpa_2netcdf.py).  The reforecast data was in turn generated by 
ESRL/PSD staff (Hamill, T. M., G. T. Bates, J. S. Whitaker, D. R. Murray, M. Fiorino, 
T. J. Galarneau, Jr., Y. Zhu, and W. Lapenta, 2013:  NOAA's second-generation global
medium-range ensemble reforecast data set. Bull Amer. Meteor. Soc., 94, 1553-1565.).
CCPA was downloaded via ftp from the NCEP/EMC web site:  To access files, ftp -i to 
ftp.emc.ncep.noaa.gov/cd_wmb/yluo/CCPA_v1/0.125d and download those of interest
to your local directory.  In the ppn_analog_ccpa_supplocns.f90 file, you will need
to edit the variables "input_data_directory" and while you're at it the 
"output_data_directory".  For the makefile, it is likely that you will have 
to update it to point toward the locations where you have netCDF and HDF5 libraries 
and to change compilers to one appropriate for your system.  If you don't have the 
netCDF and HDF5 libraries, you or a sys admin will have to find them on the web 
and have them installed.

NOTE about use of the analog methodology:  the method herein was "tuned" to give
a reasonable result with 10 supplemental locations.  Experience has illuminated that
a different number of analog members may be optimal for estimating probabilities 
when sample size changes, be it through a longer or shorter training period or
through the use of fewer or greater numbers of supplemental locations.  The number
of analog members used is set in analog_forecast_supp_v2.f90, and if you are playing
with the analog approach and intend to use fewer than 10 supplemental locations, 
consider fiddling with the code to estimate probabilities from a smaller number of 
analog members.



Data precursors to the analog code:
==================================

The analog code reads in forecast and CCPA analysis from pre-generated netCDF files.
The python script to load the data from grib files into one compact netCDF file 
is the script precip_forecast_ccpa_2netcdf.py .  We assumed it would be 
preferable for us to pre-generate them, so for a wide variety of lead times, 
these files have been put upon the ftp server ftp.cdc.noaa.gov .  You will find 
these netCDF files in the directory /Projects/Reforecast2/netcdf , with names like
/Rf2_tests/ccpa/netcdf/refcstv2_precip_ccpav3_012_to_024.nc , where 012 and 024
indicate the beginning and end lead times for these forecasts, in hours.
If you are not familiar with netCDF files, you should probably scan the web for
information on their implicit format, their advantages, and libraries for using
the data, be it in fortran or python.

Some other precursor data is needed to run the analog methodology herein.  In 
particular, pre-defined "supplemental" data locations are needed, which are generated 
by the Fortran 90 program compute_precip_analog_locations_ccpa5.x, with makefile 
make_compute_precip_analog_locations_ccpa5 .  This program requires that cumulative
distribution functions for CCPA precipitation be pre-generated, and these were 
generated using the python script compute_precip_CCPAgrid_cdfs.py .  The output 
supplemental locations file (a fortran unformatted file) is also available from
the ftp site in the same location, but this time with file names like
"ppn_analog_locns_ccpa5_MMM_024_to_048.dat" where MMM is the month (Jan, Feb, etc).
Actually, all analog forecast lead times use the supplemental locations files
above, calculated for the 024 to 048 h lead time, even if the forecast lead
time in question is different.

As mentioned above, the supplemental locations code above in turn needs 
one particular set of data pre-calculated, specifically cumulative density function 
information.  This was generated using the python script 
compute_precip_CCPAgrid_cdfs.py, and the output is on the ftp site in the 
same directory, but this time with file names like
refcstv2_apcp_CCPAgrid_CDF_fhour024_to_048_MMM.nc, where again MMM refers to 
months like Jan, Feb, etc.  A simple display of some supplemental locations can
be generated using the python code plot_supplemental_locations_v5.py.  It is 
invoked by 

> python plot_supplemental_locations_v5.py Jan

Here the month was Jan (January).  Different supplemental locations were generated
for different months of the year, presuming the precipitation climatology differed
from one month to the next.



Code for generating probabilities from the raw GEFS ensemble.
=============================================================

There is also a program called ppn_rawens_ccpa.f90 in the distribution that will
generate probabilities from the raw ensemble.  You may find it to your advantage
to run this as a reference standard against which to compare the analog approach
and any internally generated post-processing methodologies that you may have.


Plotting forecast output
========================

As outputs, the analog methodology of ppn_analog_ccpa_supplocns.x generates 
probabilistic precipitation forecasts which, for the initial version, are output 
as fortran direct-access files.  See the code for details on its format.  The 
program also generates reliability and CRPSS skill statistics.  Both the output 
forecast and skill information can be visualized. A python script is included for 
visualization of the forecasts that were saved to file.  This script is 
plot_refcstprobs_postprocessed.py, and it is run with the command 

> python plot_refcstprobs_postprocessed.py YYYYMMDDHH cleadb cleade cthresh nsupp

where YYYYMMDDHH is the initial date/time, cleadb and cleade are the initial and
final lead times of the forecast accumulation period in hours, e.g., 012 and 024. 
cthresh is the event threshold (pick from 'POP', '1mm', '2p5mm', '5mm', '10mm', 
'25mm', '50mm', 'q50', 'q67', 'q80', 'q90', 'q95', where qXX is the XXth percentile
of the climatological distribution for that month of the year).  nsupp is the number
of supplemental locations.


Plotting reliability diagrams and skill scores
==============================================

The python script reliability_analog_ccpa.py will generate reliability diagrams,
assuming that you have already generated both analog PQPF and raw ensemble PQPF,
and assuming that you have the relevant python libraries. It can be invoked by

> python reliability_analog_ccpa.py cleadb cleade cthresh

where cleadb and cleade are the beginning and end lead time in hours, e.g. 012
to 024, and cthresh is the event threshold (# acceptable values are 'POP',
'1mm','2p5mm','5mm','10mm','25mm','50mm', 'q50','q67','q80','q90','q95')
where q95 is the 95th percentile of the climatological distribution, which is
calculated independently for each month of the year and each grid point.

Suppose you have now calculated skill scores, analog and raw, over a large
variety of forecast lead times, here including 012-024, 036-048, 060-072,
084-096, 108-120, 132-144, 156-168, and 180-192.  Then the python script 

will generate plots of the annual variations in skill score for these lead times.





